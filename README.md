# An Evaluation of Open Source Real World Bugs via Popular Bug Datasets


## About the project:


In this project, we benchmarked the complexity of real-world bugs from open-source Maven and Gradle projects, utilizing popular bug datasets such as [Defects4J](https://github.com/rjust/defects4j), [BugSwarm](https://www.bugswarm.org), [Bears](https://github.com/bears-bugs/bears-benchmark), and [QuixBugs](https://github.com/jkoppel/QuixBugs). We evaluated the quality of test suites generated by automated test generation tools, specifically [Randoop](https://randoop.github.io/randoop/) and [Evosuite](https://www.evosuite.org/documentation/). To analyze code statement coverage for these bugs, we employed tools such as [Clover](https://openclover.org), [IntelliJ IDEA](https://www.jetbrains.com/help/idea/code-coverage.html), and [JaCoCo](https://www.eclemma.org/jacoco/). Using the coverage reports, we calculated suspiciousness scores for code statements to identify lines of code with the highest likelihood of containing bugs.

The primary challenge in this project was managing the substantial volume of bugs and the extensive codebases of various open-source projects, such as FreeChart, Commons-CLI, and Gson, across different datasets. We developed numerous automation scripts to identify bug-inducing commits, generate diff patches between buggy and fixed code versions, benchmark code complexity, produce tests and coverage reports, and localize bugs using suspiciousness scores.


**Benchmarks Used:**

- CChange: Number of classes changed/added/deleted to patch the bug 
- MChange: Number of methods changed/added/deleted to patch the bug
- LChange: Number of lines changed/added/deleted to patch the bug
- LD: Levenshtein edit distance between buggy and patched files
- CB: Cyclomatic complexity of buggy files
- CP: Cyclomatic complexity of patched files
- CC: Complexity change: |CB-CP|
- [CodeBLEU](https://github.com/k4black/codebleu): a weighted combination of n-gram match (BLEU), weighted n-gram match (BLEU-weighted), AST match and data-flow match scores.


## Running Benchmarks on Datasets
- Make sure you have installed <b><i>lizard, Levenshtein and codebleu</i></b> packages using pip.
- cd into this folder (CS527-team11/Scripts) and execute the benchmark script using
  
  ```bash
  python3 Scripts/Benchmark.py [bug-datset] [bug-name] [metric]
  ```
- Ensure that the <i>bug-name</i> is present in the Bugs folder of the corresponding dataset and the metric is one of <i>"CChange", "MChange", "LChange", "LD", "CB", "CP", "CC", "CodeBLEU"</i>


## How to Reproduce Bug Localization Results for the Project?

### Defects4J

For Defects4J the bugs are localized by doing the following. We have used Clover reports to get test coverage metrics for the above bugs.

Update and Running Selenium Python Script to retrieve the statement wise metrics of tests passing and failing.
   1. For this go to [./Scripts/defects4j_scripts/bug_localization/defects4j_selenium_clover_get_susp.py](./Scripts/defects4j_scripts/bug_localization/defects4j_selenium_clover_get_susp.py).
   2. Update hard coded local machine paths:
      1. Download and update selenium chrome driver path.
      2. Update the location of the buggy and fixed versions of involved files in our bugs of interest (they are currently hardcoded to local machine system paths).
      3. Update locations of the index.html files for bugs of interest.
   3. Now run the selenium python script.
   4. Once the chrome window opens up, navigate to the modified files (script waits till it sees a source file page in the clover website). Then auto clicks and reads the statement wise test coverage results. Then computes Suspiciousness, AR, FR and writes them to a CSV file.

### QuixBugs

For QuixBugs, the results can be reproduced by running QuixBugs_rank_statements.py.
1. cd into Scripts folder
2. Replace Buggy_directory and xml_location with your local paths in QuixBugs_rank_statements.py file. Only the prefix upto CS527-team11 has to be changed.
3. Run the script with
   ```bash
   python3 QuixBugs_rank_statements.py
   ```
4. The results will be displayed as well as appended to BL-Results.csv

### Bears

For Bears, the results can be reproduced by running Bears_Rank_Script.py.
1. cd into Scripts folder
2. Replace tree path with your local path for target Bug's xml file in Bears_Rank_Script.py file.
3. Run the script with
   ```bash
   python3 Bears_Rank_Script.py
   ```
4. The results will be appended to coverage.csv
