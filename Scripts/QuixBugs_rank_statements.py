import os
import time
import random
import subprocess
import xml.dom.minidom

Buggy_directory = '/Users/ajitesh/dev/CS527-team11/QuixBugs/{}/Buggy-Version'
xml_location = '/Users/ajitesh/dev/CS527-team11/QuixBugs/{}/Buggy-Version/build/reports/jacoco/test/jacocoTestReport.xml'

Bugs = ['LEVENSHTEIN','BUCKETSORT','DETECT_CYCLE','FIND_IN_SORTED','FLATTEN','KHEAPSORT','KNAPSACK']

def get_failing_and_passing_tests():
    failing_tests, passing_tests = [], []
    randoop_count = 0
    test_command1 = f"gradle clean"
    test_output1 = subprocess.run(test_command1, shell=True, capture_output=True, text=True)
    test_command = f"gradle -Dorg.gradle.jvmargs='-Xmx3g' allCoverage"
    test_output = subprocess.run(test_command, shell=True, capture_output=True, text=True)
    tests_logs = [line for line in test_output.stdout.split("\n") if ">" in line and ("PASSED" in line or "FAILED" in line)]
    for log in tests_logs:
        words = log.split(" ")
        if '.' in words[2] or len(words)<4:
            continue
        test = words[0]+'.'+words[2]
        if words[3]=="FAILED":
            failing_tests.append(test)
        else:
            if "Randoop" in test:
                # Each bug had 15000 tests generated by Randoop. We are randomly selecting  only 100 of them. 
                if random.random()>0.3:
                    continue
                if randoop_count>100:
                    continue
                randoop_count+=1
                
            passing_tests.append(test)
    return failing_tests, passing_tests

def get_statement_pass_and_fail_count(failing_tests, passing_tests, bug):
    passed_test_count_per_statement = {}
    failed_test_count_per_statement = {}
    test_command1 = f"gradle clean"
    test_output1 = subprocess.run(test_command1, shell=True, capture_output=True, text=True)
    for test in failing_tests:
        
        test_command = f"gradle -Dorg.gradle.jvmargs='-Xmx3g' allCoverage --tests {test}"
        os.system(test_command)
        xml_doc = xml.dom.minidom.parse(xml_location.format(bug))
        packages = xml_doc.getElementsByTagName("package")
        java_programs_package = [package for package in packages if package.getAttribute("name") == "java_programs"][0]
        classes = java_programs_package.getElementsByTagName("sourcefile")
        bug_class = [class_ for class_ in classes if class_.getAttribute("name") == f"{bug}.java"][0]
        for line in bug_class.getElementsByTagName("line"):
            line_number = int(line.getAttribute("nr"))
            covered = int(line.getAttribute("ci"))
            if line_number not in failed_test_count_per_statement:
                failed_test_count_per_statement[line_number] = 0
            if covered > 0:
                failed_test_count_per_statement[line_number] += 1

    for test in passing_tests:
        
        test_command = f"gradle -Dorg.gradle.jvmargs='-Xmx3g' allCoverage --tests {test}"
        subprocess.run(test_command, shell=True, capture_output=True, text=True)
        xml_doc = xml.dom.minidom.parse(xml_location.format(bug))
        packages = xml_doc.getElementsByTagName("package")
        java_programs_package = [package for package in packages if package.getAttribute("name") == "java_programs"][0]
        classes = java_programs_package.getElementsByTagName("sourcefile")
        bug_class = [class_ for class_ in classes if class_.getAttribute("name") == f"{bug}.java"][0]
        for line in bug_class.getElementsByTagName("line"):
            line_number = int(line.getAttribute("nr"))
            covered = int(line.getAttribute("ci"))
            if line_number not in passed_test_count_per_statement:
                passed_test_count_per_statement[line_number] = 0
            if covered > 0:
                passed_test_count_per_statement[line_number] += 1

    return passed_test_count_per_statement, failed_test_count_per_statement

def get_suspiciousness_per_statement(passed_test_count_per_statement, failed_test_count_per_statement, total_pass, total_fail):
    suspiciousness_per_statement = {}
    for line_number in passed_test_count_per_statement:
        numerator = failed_test_count_per_statement[line_number]/total_fail
        denominator = (failed_test_count_per_statement[line_number]/total_fail) + (passed_test_count_per_statement[line_number]/total_pass)
        if denominator == 0:
            suspiciousness_per_statement[line_number] = 0
        else:
            suspiciousness_per_statement[line_number] = numerator/denominator
    return suspiciousness_per_statement

def get_buggy_file_path(bug_name):
    return f"/Users/ajitesh/dev/CS527-team11/QuixBugs/{bug_name}/Buggy-Version/java_programs/{bug_name}.java"

def get_patched_file_path(bug_name):
    return f"/Users/ajitesh/dev/CS527-team11/QuixBugs/{bug_name}/Patched-Version/correct_java_programs/{bug_name}.java"

def read_file(bug_name):
    with open(get_buggy_file_path(bug_name), "r") as b_file:
        buggy = b_file.read()

    with open(get_patched_file_path(bug_name), "r") as p_file:
        patched = p_file.read()

    return buggy, patched

def get_involved_statements(bug):
    involved_statements = []
    buggy, patched = read_file(bug)
    buggy_lines = buggy.split("\n")
    patched_lines = patched.split("\n")
    for i in range(1,len(buggy_lines)):
        if buggy_lines[i] not in patched_lines:
            involved_statements.append(i+1)

    return involved_statements

def get_average_and_first_rank(involved_statements, ranked_suspiciousness_per_statement):
    
    first_rank = 999
    involved_ranks_sum = 0
    i = 1
    rank = i
    prev_rank = 0
    prev_score = -1
    for line_number, score in ranked_suspiciousness_per_statement.items():
        if score != prev_score:
            rank = i
        else:
            rank = prev_rank
        prev_score = score
        prev_rank = rank
        if line_number in involved_statements:
            involved_ranks_sum += rank
            if rank < first_rank:
                first_rank = rank
        i += 1

    average_rank = involved_ranks_sum / len(involved_statements)
    
    return first_rank, average_rank

def create_csv_if_not_exists():
    if not os.path.exists("/Users/ajitesh/dev/CS527-team11/BL-Results.csv"):
        with open("/Users/ajitesh/dev/CS527-team11/BL-Results.csv", "w") as f:
            f.write("Repo name, Bug ID, AR, FR\n")

    

if __name__ == "__main__":
    report=""

    for bug in Bugs:
        source_buggy_directory = Buggy_directory.format(bug)

        print("Starting for ", bug)

        if not os.path.isdir(source_buggy_directory):
            print(f"Source directory '{source_buggy_directory}' not found.")
            exit(1)

        os.chdir(source_buggy_directory)

        failing_tests, passing_tests = get_failing_and_passing_tests()
        total_pass = len(passing_tests)
        total_fail = len(failing_tests)

        print("Total pass", total_pass)
        print("Total fail", total_fail)

        passed_test_count_per_statement, failed_test_count_per_statement = get_statement_pass_and_fail_count(failing_tests, passing_tests, bug)
        print("pass count", passed_test_count_per_statement)
        print("fail count", failed_test_count_per_statement)

        suspiciousness_per_statement = get_suspiciousness_per_statement(passed_test_count_per_statement, failed_test_count_per_statement, total_pass, total_fail)
        print("Suspiciousness per statement", suspiciousness_per_statement)
        ranked_suspiciousness_per_statement = dict(sorted(suspiciousness_per_statement.items(), key=lambda item: item[1], reverse=True))
        print(bug, "Ranked suspiciousness per statement", ranked_suspiciousness_per_statement)
        report+=f"{bug} Ranked suspiciousness per statement: {ranked_suspiciousness_per_statement}\n"
        involved_statements = get_involved_statements(bug)
        print(bug, "Involved statements", involved_statements)
        report+=f"{bug} Involved statements: {involved_statements}\n"

        first_rank, average_rank = get_average_and_first_rank(involved_statements, ranked_suspiciousness_per_statement)
        print(bug, "First rank", first_rank)
        report+=f"{bug} First rank: {first_rank}\n"
        print(bug, "Average rank", average_rank)
        report+=f"{bug} Average rank: {average_rank}\n"
        create_csv_if_not_exists()

        with open("/Users/ajitesh/dev/CS527-team11/BL-Results.csv", "a") as f:
            f.write(f"QuixBugs,{bug},{average_rank},{first_rank}\n")

        test_command1 = f"gradle clean"
        test_output1 = subprocess.run(test_command1, shell=True, capture_output=True, text=True)
        test_command = f"rm -r /Users/ajitesh/dev/CS527-team11/QuixBugs/{bug}/Buggy-Version/JacocoCoverage/"
        test_output = subprocess.run(test_command, shell=True, capture_output=True, text=True)


        print("Completed statement ranking for ", bug)
        print(report)


print("Script completed successfully.")
print(report)



# Script completed successfully.
# LEVENSHTEIN Ranked suspiciousness per statement: {16: 1.0, 17: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 14: 0.7777777777777779, 15: 0.7777777777777779, 12: 0}
# LEVENSHTEIN Involved statements: [17]
# LEVENSHTEIN First rank: 1
# LEVENSHTEIN Average rank: 1.0
# BUCKETSORT Ranked suspiciousness per statement: {20: 0.6170212765957447, 21: 0.6170212765957447, 22: 0.6170212765957447, 23: 0.6170212765957447, 24: 0.6170212765957447, 25: 0.6170212765957447, 27: 0.6170212765957447, 17: 0.5979381443298969, 18: 0.5979381443298969, 16: 0.5858585858585859, 15: 0.5686274509803921, 13: 0}
# BUCKETSORT Involved statements: [22]
# BUCKETSORT First rank: 1
# BUCKETSORT Average rank: 1.0
# DETECT_CYCLE Ranked suspiciousness per statement: {21: 0.6923076923076923, 22: 0.6923076923076923, 24: 0.6923076923076923, 14: 0.6428571428571428, 15: 0.6428571428571428, 18: 0.6428571428571428, 12: 0, 19: 0.0, 25: 0.0}
# DETECT_CYCLE Involved statements: [18]
# DETECT_CYCLE First rank: 4
# DETECT_CYCLE Average rank: 4.0
# FIND_IN_SORTED Ranked suspiciousness per statement: {19: 0.8170731707317074, 16: 0.7701149425287357, 17: 0.7701149425287357, 13: 0.5677966101694916, 11: 0, 14: 0.0, 18: 0.0, 20: 0.0, 22: 0.0, 27: 0.0}
# FIND_IN_SORTED Involved statements: [20]
# FIND_IN_SORTED First rank: 5
# FIND_IN_SORTED Average rank: 5.0
# FLATTEN Ranked suspiciousness per statement: {14: 0.8, 15: 0.8, 16: 0.8, 17: 0.8, 18: 0.5714285714285715, 19: 0.4, 23: 0.4, 24: 0.4, 12: 0.0, 21: 0, 26: 0}
# FLATTEN Involved statements: [21, 26]
# FLATTEN First rank: 9
# FLATTEN Average rank: 9.0
# KHEAPSORT Ranked suspiciousness per statement: {23: 0.5523809523809524, 24: 0.5523809523809524, 34: 0.5523809523809524, 22: 0.5471698113207547, 26: 0.5471698113207547, 27: 0.5471698113207547, 28: 0.5471698113207547, 29: 0.5471698113207547, 30: 0.5471698113207547, 31: 0.5471698113207547, 33: 0.5471698113207547, 37: 0.5471698113207547, 21: 0.5272727272727272, 12: 0}
# KHEAPSORT Involved statements: [27]
# KHEAPSORT First rank: 4
# KHEAPSORT Average rank: 4.0
# KNAPSACK Ranked suspiciousness per statement: {34: 0.5789473684210527, 30: 0.5739130434782609, 31: 0.5739130434782609, 39: 0.5739130434782609, 22: 0.5689655172413793, 23: 0.5689655172413793, 27: 0.5689655172413793, 28: 0.5689655172413793, 15: 0.5593220338983051, 16: 0.5593220338983051, 17: 0.5593220338983051, 19: 0.5593220338983051, 21: 0.5593220338983051, 25: 0.5593220338983051, 13: 0.0}
# KNAPSACK Involved statements: [30]
# KNAPSACK First rank: 2
# KNAPSACK Average rank: 2.0