diff --git a/docs/source/en/api/pipelines/fabric.md b/docs/source/en/api/pipelines/fabric.md
new file mode 100644
index 00000000..e37e91f5
--- /dev/null
+++ b/docs/source/en/api/pipelines/fabric.md
@@ -0,0 +1,31 @@
+## changes required
+<!--Copyright 2023 The HuggingFace Team. All rights reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
+the License. You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
+an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
+specific language governing permissions and limitations under the License.
+-->
+
+# Text-to-Image Generation
+
+## FabricPipeline
+
+[FABRIC: Personalizing Diffusion Models with Iterative Feedback](https://huggingface.co/papers/2307.10159) (FABRIC) is by Dimitri von Rütte, Elisabetta Fedele, Jonathan Thomm and Lukas Wolf
+
+FABRIC is training-free approach that conditions the diffusion process on a set of feedback images, applicable to a wide range of popular diffusion models, created by the researchers and engineers from [ETH Zürich, Switzerland](https://github.com/sd-fabric). The [`FabricPipeline`] is capable of generating photo-realistic images given any text input using Stable Diffusion and finetune them on the basis of feedback.
+
+The original codebase can be found here: 
+- *FABRIC*: [sd-fabric/fabric](https://github.com/sd-fabric/fabric)
+
+Available Checkpoints are:
+- *dreamlike-photoreal-2.0 (512x512 resolution)* [dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)
+
+[[autodoc]] FabricPipeline
+	- all
+	- __call__
+
diff --git a/src/diffusers/pipelines/fabric/pipeline_fabric.py b/src/diffusers/pipelines/fabric/pipeline_fabric.py
index 5f4dd555..163e284c 100644
--- a/src/diffusers/pipelines/fabric/pipeline_fabric.py
+++ b/src/diffusers/pipelines/fabric/pipeline_fabric.py
@@ -50,7 +50,6 @@ class CrossAttnProcessor:
         weights=None,  # shape: (batch_size, sequence_length)
         lora_scale=1.0,
     ):
-        print("in")
         batch_size, sequence_length, _ = (
             hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
         )
@@ -487,7 +486,6 @@ class FabricPipeline(DiffusionPipeline):
                     noise = torch.randn_like(z_ref)
                     if isinstance(self.scheduler, EulerAncestralDiscreteScheduler):
                         z_ref_noised = (alpha_hat**0.5 * z_ref + (1 - alpha_hat) ** 0.5 * noise).type(dtype)
-                        print("here")
                     else:
                         z_ref_noised = self.scheduler.add_noise(z_ref, noise, t)
 
diff --git a/src/diffusers/utils/dummy_torch_and_transformers_objects.py b/src/diffusers/utils/dummy_torch_and_transformers_objects.py
index dbddf6e2..83ac22ba 100644
--- a/src/diffusers/utils/dummy_torch_and_transformers_objects.py
+++ b/src/diffusers/utils/dummy_torch_and_transformers_objects.py
@@ -62,6 +62,21 @@ class CycleDiffusionPipeline(metaclass=DummyObject):
         requires_backends(cls, ["torch", "transformers"])
 
 
+class FabricPipeline(metaclass=DummyObject):
+    _backends = ["torch", "transformers"]
+
+    def __init__(self, *args, **kwargs):
+        requires_backends(self, ["torch", "transformers"])
+
+    @classmethod
+    def from_config(cls, *args, **kwargs):
+        requires_backends(cls, ["torch", "transformers"])
+
+    @classmethod
+    def from_pretrained(cls, *args, **kwargs):
+        requires_backends(cls, ["torch", "transformers"])
+
+
 class IFImg2ImgPipeline(metaclass=DummyObject):
     _backends = ["torch", "transformers"]
 
